<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en-US"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>Train a model using Lightweight Temporal Self-Attention Encoder — sits_lighttae • sits</title><script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet"><script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/IBM_Plex_Serif-0.4.10/font.css" rel="stylesheet"><link href="../deps/IBM_Plex_Mono-0.4.10/font.css" rel="stylesheet"><link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet"><link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet"><script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Train a model using Lightweight Temporal Self-Attention Encoder — sits_lighttae"><meta name="description" content="Implementation of Light Temporal Attention Encoder (L-TAE)
for satellite image time series. This is a lightweight version of the
temporal attention encoder proposed by Garnot et al. For the TAE,
please see sits_tae.
TAE is a simplified version of the well-known self-attention architeture
which is used in large language models.
Its modified self-attention scheme that uses the input
embeddings as values. TAE defines a single master query for each sequence,
computed from the temporal average of the queries. This master query is compared
to the sequence of keys to produce a single attention mask
used to weight the temporal mean of values into a single feature vector.
The lightweight version of TAE further simplifies the TAE model.
It defines master query of each head as a model parameter instead
of the results of a linear layer, as is done it TAE.
The authors argue that such simplification reduces the number of parameters,
while the lack of flexibility is compensated by the larger number of available heads."><meta property="og:description" content="Implementation of Light Temporal Attention Encoder (L-TAE)
for satellite image time series. This is a lightweight version of the
temporal attention encoder proposed by Garnot et al. For the TAE,
please see sits_tae.
TAE is a simplified version of the well-known self-attention architeture
which is used in large language models.
Its modified self-attention scheme that uses the input
embeddings as values. TAE defines a single master query for each sequence,
computed from the temporal average of the queries. This master query is compared
to the sequence of keys to produce a single attention mask
used to weight the temporal mean of values into a single feature vector.
The lightweight version of TAE further simplifies the TAE model.
It defines master query of each head as a model parameter instead
of the results of a linear layer, as is done it TAE.
The authors argue that such simplification reduces the number of parameters,
while the lack of flexibility is compensated by the larger number of available heads."></head><body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">sits</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">1.5.3-1</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto"><li class="active nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="nav-item"><a class="nav-link" href="../news/index.html">Changelog</a></li>
<li class="nav-item"><a class="nav-link" href="https://e-sensing.github.io/sitsbook/">Book</a></li>
      </ul><ul class="navbar-nav"><li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json"></form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/e-sensing/sits/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul></div>


  </div>
</nav><div class="container template-reference-topic">
<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">

      <h1>Train a model using Lightweight Temporal Self-Attention Encoder</h1>
      <small class="dont-index">Source: <a href="https://github.com/e-sensing/sits/blob/v1.5.3-1/R/sits_lighttae.R" class="external-link"><code>R/sits_lighttae.R</code></a></small>
      <div class="d-none name"><code>sits_lighttae.Rd</code></div>
    </div>

    <div class="ref-description section level2">
    <p>Implementation of Light Temporal Attention Encoder (L-TAE)
for satellite image time series. This is a lightweight version of the
temporal attention encoder proposed by Garnot et al. For the TAE,
please see <code><a href="sits_tae.html">sits_tae</a></code>.</p>
<p>TAE is a simplified version of the well-known self-attention architeture
which is used in large language models.
Its modified self-attention scheme that uses the input
embeddings as values. TAE defines a single master query for each sequence,
computed from the temporal average of the queries. This master query is compared
to the sequence of keys to produce a single attention mask
used to weight the temporal mean of values into a single feature vector.</p>
<p>The lightweight version of TAE further simplifies the TAE model.
It defines master query of each head as a model parameter instead
of the results of a linear layer, as is done it TAE.
The authors argue that such simplification reduces the number of parameters,
while the lack of flexibility is compensated by the larger number of available heads.</p>
    </div>

    <div class="section level2">
    <h2 id="ref-usage">Usage<a class="anchor" aria-label="anchor" href="#ref-usage"></a></h2>
    <div class="sourceCode"><pre class="sourceCode r"><code><span><span class="fu">sits_lighttae</span><span class="op">(</span></span>
<span>  samples <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  samples_validation <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  epochs <span class="op">=</span> <span class="fl">150L</span>,</span>
<span>  batch_size <span class="op">=</span> <span class="fl">128L</span>,</span>
<span>  validation_split <span class="op">=</span> <span class="fl">0.2</span>,</span>
<span>  optimizer <span class="op">=</span> <span class="fu">torch</span><span class="fu">::</span><span class="va"><a href="https://torch.mlverse.org/docs/reference/optim_adamw.html" class="external-link">optim_adamw</a></span>,</span>
<span>  opt_hparams <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>lr <span class="op">=</span> <span class="fl">5e-04</span>, eps <span class="op">=</span> <span class="fl">1e-08</span>, weight_decay <span class="op">=</span> <span class="fl">7e-04</span><span class="op">)</span>,</span>
<span>  lr_decay_epochs <span class="op">=</span> <span class="fl">50L</span>,</span>
<span>  lr_decay_rate <span class="op">=</span> <span class="fl">1</span>,</span>
<span>  patience <span class="op">=</span> <span class="fl">20L</span>,</span>
<span>  min_delta <span class="op">=</span> <span class="fl">0.01</span>,</span>
<span>  seed <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  verbose <span class="op">=</span> <span class="cn">FALSE</span></span>
<span><span class="op">)</span></span></code></pre></div>
    </div>

    <div class="section level2">
    <h2 id="arguments">Arguments<a class="anchor" aria-label="anchor" href="#arguments"></a></h2>


<dl><dt id="arg-samples">samples<a class="anchor" aria-label="anchor" href="#arg-samples"></a></dt>
<dd><p>Time series with the training samples
(tibble of class "sits").</p></dd>


<dt id="arg-samples-validation">samples_validation<a class="anchor" aria-label="anchor" href="#arg-samples-validation"></a></dt>
<dd><p>Time series with the validation samples
(tibble of class "sits").
If <code>samples_validation</code> parameter is provided,
<code>validation_split</code> is ignored.</p></dd>


<dt id="arg-epochs">epochs<a class="anchor" aria-label="anchor" href="#arg-epochs"></a></dt>
<dd><p>Number of iterations to train the model
(integer, min = 1, max = 20000).</p></dd>


<dt id="arg-batch-size">batch_size<a class="anchor" aria-label="anchor" href="#arg-batch-size"></a></dt>
<dd><p>Number of samples per gradient update
(integer, min = 16L, max = 2048L)</p></dd>


<dt id="arg-validation-split">validation_split<a class="anchor" aria-label="anchor" href="#arg-validation-split"></a></dt>
<dd><p>Fraction of training data
to be used as validation data.</p></dd>


<dt id="arg-optimizer">optimizer<a class="anchor" aria-label="anchor" href="#arg-optimizer"></a></dt>
<dd><p>Optimizer function to be used.</p></dd>


<dt id="arg-opt-hparams">opt_hparams<a class="anchor" aria-label="anchor" href="#arg-opt-hparams"></a></dt>
<dd><p>Hyperparameters for optimizer:
<code>lr</code> : Learning rate of the optimizer
<code>eps</code>: Term added to the denominator
     to improve numerical stability.
<code>weight_decay</code>:       L2 regularization rate.</p></dd>


<dt id="arg-lr-decay-epochs">lr_decay_epochs<a class="anchor" aria-label="anchor" href="#arg-lr-decay-epochs"></a></dt>
<dd><p>Number of epochs to reduce learning rate.</p></dd>


<dt id="arg-lr-decay-rate">lr_decay_rate<a class="anchor" aria-label="anchor" href="#arg-lr-decay-rate"></a></dt>
<dd><p>Decay factor for reducing learning rate.</p></dd>


<dt id="arg-patience">patience<a class="anchor" aria-label="anchor" href="#arg-patience"></a></dt>
<dd><p>Number of epochs without improvements until
training stops.</p></dd>


<dt id="arg-min-delta">min_delta<a class="anchor" aria-label="anchor" href="#arg-min-delta"></a></dt>
<dd><p>Minimum improvement in loss function
to reset the patience counter.</p></dd>


<dt id="arg-seed">seed<a class="anchor" aria-label="anchor" href="#arg-seed"></a></dt>
<dd><p>Seed for random values.</p></dd>


<dt id="arg-verbose">verbose<a class="anchor" aria-label="anchor" href="#arg-verbose"></a></dt>
<dd><p>Verbosity mode (TRUE/FALSE). Default is FALSE.</p></dd>

</dl></div>
    <div class="section level2">
    <h2 id="value">Value<a class="anchor" aria-label="anchor" href="#value"></a></h2>
    <p>A fitted model to be used for classification of data cubes.</p>
    </div>
    <div class="section level2">
    <h2 id="note">Note<a class="anchor" aria-label="anchor" href="#note"></a></h2>
    <p><code>sits</code> provides a set of default values for all classification models.
These settings have been chosen based on testing by the authors.
Nevertheless, users can control all parameters for each model.
Novice users can rely on the default values,
while experienced ones can fine-tune deep learning models
using <code><a href="sits_tuning.html">sits_tuning</a></code>.</p>
<p>This function is based on the paper by Vivien Garnot referenced below
and code available on github at
<a href="https://github.com/VSainteuf/lightweight-temporal-attention-pytorch" class="external-link">https://github.com/VSainteuf/lightweight-temporal-attention-pytorch</a>
If you use this method, please cite the original TAE and the LTAE paper.</p>
<p>We also used the code made available by Maja Schneider in her work with
Marco Körner referenced below and available at
<a href="https://github.com/maja601/RC2020-psetae" class="external-link">https://github.com/maja601/RC2020-psetae</a>.</p>
    </div>
    <div class="section level2">
    <h2 id="references">References<a class="anchor" aria-label="anchor" href="#references"></a></h2>
    <p>Vivien Garnot, Loic Landrieu, Sebastien Giordano, and Nesrine Chehata,
"Satellite Image Time Series Classification with Pixel-Set Encoders
and Temporal Self-Attention",
2020 Conference on Computer Vision and Pattern Recognition.
pages 12322-12331.
<a href="https://doi.org/10.1109/CVPR42600.2020.01234" class="external-link">doi:10.1109/CVPR42600.2020.01234</a></p>
<p>Vivien Garnot, Loic Landrieu,
"Lightweight Temporal Self-Attention  for Classifying
Satellite Images Time Series",
arXiv preprint arXiv:2007.00586, 2020.</p>
<p>Schneider, Maja; Körner, Marco,
"[Re] Satellite Image Time Series Classification
with Pixel-Set Encoders and Temporal Self-Attention."
ReScience C 7 (2), 2021.
<a href="https://doi.org/10.5281/zenodo.4835356" class="external-link">doi:10.5281/zenodo.4835356</a></p>
    </div>
    <div class="section level2">
    <h2 id="author">Author<a class="anchor" aria-label="anchor" href="#author"></a></h2>
    <p>Gilberto Camara, <a href="mailto:gilberto.camara@inpe.br">gilberto.camara@inpe.br</a></p>
<p>Rolf Simoes, <a href="mailto:rolfsimoes@gmail.com">rolfsimoes@gmail.com</a></p>
<p>Charlotte Pelletier, <a href="mailto:charlotte.pelletier@univ-ubs.fr">charlotte.pelletier@univ-ubs.fr</a></p>
    </div>

    <div class="section level2">
    <h2 id="ref-examples">Examples<a class="anchor" aria-label="anchor" href="#ref-examples"></a></h2>
    <div class="sourceCode"><pre class="sourceCode r"><code><span class="r-in"><span><span class="kw">if</span> <span class="op">(</span><span class="fu"><a href="sits_run_examples.html">sits_run_examples</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span></span>
<span class="r-in"><span>    <span class="co"># create a lightTAE model</span></span></span>
<span class="r-in"><span>    <span class="va">torch_model</span> <span class="op">&lt;-</span> <span class="fu"><a href="sits_train.html">sits_train</a></span><span class="op">(</span><span class="va">samples_modis_ndvi</span>, <span class="fu">sits_lighttae</span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span></span>
<span class="r-in"><span>    <span class="co"># plot the model</span></span></span>
<span class="r-in"><span>    <span class="fu"><a href="plot.html">plot</a></span><span class="op">(</span><span class="va">torch_model</span><span class="op">)</span></span></span>
<span class="r-in"><span>    <span class="co"># create a data cube from local files</span></span></span>
<span class="r-in"><span>    <span class="va">data_dir</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/system.file.html" class="external-link">system.file</a></span><span class="op">(</span><span class="st">"extdata/raster/mod13q1"</span>, package <span class="op">=</span> <span class="st">"sits"</span><span class="op">)</span></span></span>
<span class="r-in"><span>    <span class="va">cube</span> <span class="op">&lt;-</span> <span class="fu"><a href="sits_cube.html">sits_cube</a></span><span class="op">(</span></span></span>
<span class="r-in"><span>        source <span class="op">=</span> <span class="st">"BDC"</span>,</span></span>
<span class="r-in"><span>        collection <span class="op">=</span> <span class="st">"MOD13Q1-6.1"</span>,</span></span>
<span class="r-in"><span>        data_dir <span class="op">=</span> <span class="va">data_dir</span></span></span>
<span class="r-in"><span>    <span class="op">)</span></span></span>
<span class="r-in"><span>    <span class="co"># classify a data cube</span></span></span>
<span class="r-in"><span>    <span class="va">probs_cube</span> <span class="op">&lt;-</span> <span class="fu"><a href="sits_classify.html">sits_classify</a></span><span class="op">(</span></span></span>
<span class="r-in"><span>        data <span class="op">=</span> <span class="va">cube</span>, ml_model <span class="op">=</span> <span class="va">torch_model</span>, output_dir <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/tempfile.html" class="external-link">tempdir</a></span><span class="op">(</span><span class="op">)</span></span></span>
<span class="r-in"><span>    <span class="op">)</span></span></span>
<span class="r-in"><span>    <span class="co"># plot the probability cube</span></span></span>
<span class="r-in"><span>    <span class="fu"><a href="plot.html">plot</a></span><span class="op">(</span><span class="va">probs_cube</span><span class="op">)</span></span></span>
<span class="r-in"><span>    <span class="co"># smooth the probability cube using Bayesian statistics</span></span></span>
<span class="r-in"><span>    <span class="va">bayes_cube</span> <span class="op">&lt;-</span> <span class="fu"><a href="sits_smooth.html">sits_smooth</a></span><span class="op">(</span><span class="va">probs_cube</span>, output_dir <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/tempfile.html" class="external-link">tempdir</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span></span>
<span class="r-in"><span>    <span class="co"># plot the smoothed cube</span></span></span>
<span class="r-in"><span>    <span class="fu"><a href="plot.html">plot</a></span><span class="op">(</span><span class="va">bayes_cube</span><span class="op">)</span></span></span>
<span class="r-in"><span>    <span class="co"># label the probability cube</span></span></span>
<span class="r-in"><span>    <span class="va">label_cube</span> <span class="op">&lt;-</span> <span class="fu"><a href="sits_label_classification.html">sits_label_classification</a></span><span class="op">(</span></span></span>
<span class="r-in"><span>        <span class="va">bayes_cube</span>,</span></span>
<span class="r-in"><span>        output_dir <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/tempfile.html" class="external-link">tempdir</a></span><span class="op">(</span><span class="op">)</span></span></span>
<span class="r-in"><span>    <span class="op">)</span></span></span>
<span class="r-in"><span>    <span class="co"># plot the labelled cube</span></span></span>
<span class="r-in"><span>    <span class="fu"><a href="plot.html">plot</a></span><span class="op">(</span><span class="va">label_cube</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="op">}</span></span></span>
</code></pre></div>
    </div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside></div>


    <footer><div class="pkgdown-footer-left">
  <p>Developed by Rolf Simoes, Gilberto Camara, Felipe Souza, Felipe Carlos.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.3.</p>
</div>

    </footer></div>





  </body></html>

